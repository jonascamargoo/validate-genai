import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer

def remove_urls(text):
    # Substitui qualquer URL (come√ßando com http:// ou https:// ou www.) por string vazia
    return re.sub(r'https?://\S+|www\.\S+', '', text)

def remove_special_characters(text):
    # Remove pontua√ß√µes e caracteres especiais, mantendo apenas letras e espa√ßos
    # Pode ser prejudicial para a sem√¢ntica
    return re.sub(r'[^\w\s√°√©√≠√≥√∫√£√µ√ß√Å√â√ç√ì√ö√É√ï√á]', '', text)

def remove_numbers(text):
    # Remover n√∫meros - substitui sequ√™ncias de d√≠gitos por string vazia
    return re.sub(r'\d+', '', text)

def tokenize_text(text):
    # Tokenizar - dividir o texto em palavras individuais usando tokenizador espec√≠fico para portugu√™s
    return word_tokenize(text, language='portuguese')

def remove_stopwords_and_short_tokens(tokens):
    # Remove stopwords e palavras muito curtas (menos de 3 caracteres)
    stop_words = set(stopwords.words('portuguese'))
    filtered_tokens = []
    for token in tokens:
        # Verifica se a palavra n√£o √© uma stopword e tem mais de 2 caracteres
        if token not in stop_words and len(token) > 2:
            filtered_tokens.append(token)
    
    return filtered_tokens

def apply_stemming(tokens):
    """
    Aplica stemming para reduzir palavras ao seu radical.
    
    Aplicar stemming - reduzir palavras ao seu radical. Ex: "comprei", "comprando", "compra" -> "compr". Isso ajuda a unificar varia√ß√µes de uma mesma palavra
    
    O stemmer pode prejudicar a legibilidade do texto, mas ajuda a melhorar a efic√°cia do modelo. Devo verificar com e sem o stemming, j√° que pretendo validar textos curtos
    
    """
    stemmer = RSLPStemmer()
    return [stemmer.stem(token) for token in tokens]

def pre_process_portuguese(text):
    # Fun√ß√£o principal que realiza o pr√©-processamento completo do texto.
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = remove_urls(text)
    text = remove_special_characters(text)
    text = remove_numbers(text)
    tokens = tokenize_text(text)
    tokens = remove_stopwords_and_short_tokens(tokens)
    tokens = apply_stemming(tokens) # Juntar as palavras processadas de volta em um texto √∫nico separado por espa√ßos
    return ' '.join(tokens)


# dataset real de avalia√ß√µes de produtos da B2W (Americanas, Submarino, Shoptime). Link para download: https://github.com/americanas-tech/b2w-reviews01
import pandas as pd
import os


def load_df_processed():
    """
    Carrega o dataset de avalia√ß√µes da B2W e converte as notas em categorias de sentimento.
    
    Retorna:
    - df (DataFrame): DataFrame processado com as colunas ['overall_rating', 'review_title', 'sentiment'].
    """
    # üîπ Obt√©m o diret√≥rio onde este script est√° salvo
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # üîπ Caminho absoluto do dataset
    dataset_path = os.path.join(script_dir, "B2W-Reviews01.csv")
    
    try:
        # Carrega o dataset
        df = pd.read_csv(dataset_path, encoding="utf-8")
        
        # Filtra colunas relevantes
        df_subset = df[["overall_rating", "review_text"]].copy()
        df_subset.rename(columns={"overall_rating": "sentiment"}, inplace=True)
        
        # Aplica o pr√©-processamento na coluna 'review_text' e substitui o conte√∫do original
        df_subset['processed_text'] = df_subset['review_text'].apply(pre_process_portuguese)
        
        # Remove a coluna 'review_text' e mant√©m apenas o texto processado
        df_subset.drop(columns=['review_text'], inplace=True)
        
        return df_subset

    except FileNotFoundError:
        print(f"Erro: O arquivo {dataset_path} n√£o foi encontrado.")
        return None
    
    
# Exibe o dataset processado
# df_processed = load_df_processed()
# print(df_processed.head())